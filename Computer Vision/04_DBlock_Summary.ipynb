{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_DBlock_Summary.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iraltFkw1oaz",
        "colab_type": "text"
      },
      "source": [
        "# 04 - DataBlock Summary\n",
        "\n",
        "This notebook shows how to work `DataBlock.summary()` and what it can do!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeOcHkFehdOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Run once per session\n",
        "import os\n",
        "!pip install -q feather-format kornia pyarrow wandb nbdev fastprogress  --upgrade \n",
        "!pip install torch==1.3.1\n",
        "!pip install torchvision==0.4.2\n",
        "!pip install Pillow==6.2.1 --upgrade\n",
        "!pip install git+https://github.com/fastai/fastai2\n",
        "!pip install git+https://github.com/fastai/fastcore\n",
        "os._exit(00)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S238xDV7hjDp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai2.vision.all import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7zUwbej1unD",
        "colab_type": "text"
      },
      "source": [
        "We'll use `ImageWoof` like we did in previous notebooks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN-jD8Gfhlnm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "333c78f9-ecd3-4c1a-a648-2387a822dc78"
      },
      "source": [
        "path = untar_data(URLs.IMAGEWOOF)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7jVSt1Mhx7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lbl_dict = dict(\n",
        "  n02086240= 'Shih-Tzu',\n",
        "  n02087394= 'Rhodesian ridgeback',\n",
        "  n02088364= 'Beagle',\n",
        "  n02089973= 'English foxhound',\n",
        "  n02093754= 'Australian terrier',\n",
        "  n02096294= 'Border terrier',\n",
        "  n02099601= 'Golden retriever',\n",
        "  n02105641= 'Old English sheepdog',\n",
        "  n02111889= 'Samoyed',\n",
        "  n02115641= 'Dingo'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_unk7wHeiBGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_tfms = [*aug_transforms(size=224, max_warp=0), Normalize.from_stats(*imagenet_stats, cuda=False)]\n",
        "item_tfms = Resize(128)\n",
        "bs=64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeSzaRmPhqnU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pets = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
        "                 get_items=get_image_files,\n",
        "                 splitter=RandomSplitter(),\n",
        "                 get_y=[parent_label, lbl_dict.__getitem__],\n",
        "                 item_tfms=item_tfms,\n",
        "                 batch_tfms=batch_tfms)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2TmABQE1xiF",
        "colab_type": "text"
      },
      "source": [
        "Now to run `.summary`, we need to send in what our `DataBlock` expects. In this case it's a path (think how we make our `DataLoaders` from the `DataBlock`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbrX7FrVjks1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aef840ee-121a-49a4-e23b-8633b4fc6772"
      },
      "source": [
        "pets.summary(path)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting-up type transforms pipelines\n",
            "Collecting items from /root/.fastai/data/imagewoof2\n",
            "Found 12954 items\n",
            "2 datasets of sizes 10364,2590\n",
            "Setting up Pipeline: PILBase.create\n",
            "Setting up Pipeline: parent_label -> dict.__getitem__ -> Categorize\n",
            "\n",
            "Building one sample\n",
            "  Pipeline: PILBase.create\n",
            "    starting from\n",
            "      /root/.fastai/data/imagewoof2/train/n02096294/n02096294_3256.JPEG\n",
            "    applying PILBase.create gives\n",
            "      <fastai2.vision.core.PILImage image mode=RGB size=200x200 at 0x7F2A65CA2668>\n",
            "  Pipeline: parent_label -> dict.__getitem__ -> Categorize\n",
            "    starting from\n",
            "      /root/.fastai/data/imagewoof2/train/n02096294/n02096294_3256.JPEG\n",
            "    applying parent_label gives\n",
            "      n02096294\n",
            "    applying dict.__getitem__ gives\n",
            "      Border terrier\n",
            "    applying Categorize gives\n",
            "      TensorCategory(2)\n",
            "\n",
            "Final sample: (<fastai2.vision.core.PILImage image mode=RGB size=200x200 at 0x7F2A66EA4EF0>, TensorCategory(2))\n",
            "\n",
            "\n",
            "Setting up after_item: Pipeline: Resize -> ToTensor\n",
            "Setting up before_batch: Pipeline: \n",
            "Setting up after_batch: Pipeline: IntToFloatTensor -> AffineCoordTfm -> LightingTfm -> Normalize\n",
            "Could not do one pass in your dataloader, there is something wrong in it\n",
            "\n",
            "Building one batch\n",
            "Applying item_tfms to the first sample:\n",
            "  Pipeline: Resize -> ToTensor\n",
            "    starting from\n",
            "      (<fastai2.vision.core.PILImage image mode=RGB size=200x200 at 0x7F2A66EA45F8>, TensorCategory(2))\n",
            "    applying Resize gives\n",
            "      (<fastai2.vision.core.PILImage image mode=RGB size=128x128 at 0x7F2A66EA4DA0>, TensorCategory(2))\n",
            "    applying ToTensor gives\n",
            "      (TensorImage of size 3x128x128, TensorCategory(2))\n",
            "\n",
            "Adding the next 3 samples\n",
            "\n",
            "No before_batch transform to apply\n",
            "\n",
            "Collating items in a batch\n",
            "\n",
            "Applying batch_tfms to the batch built\n",
            "  Pipeline: IntToFloatTensor -> AffineCoordTfm -> LightingTfm -> Normalize\n",
            "    starting from\n",
            "      (TensorImage of size 4x3x128x128, TensorCategory([2, 7, 7, 2]))\n",
            "    applying IntToFloatTensor gives\n",
            "      (TensorImage of size 4x3x128x128, TensorCategory([2, 7, 7, 2]))\n",
            "    applying AffineCoordTfm gives\n",
            "      (TensorImage of size 4x3x224x224, TensorCategory([2, 7, 7, 2]))\n",
            "    applying LightingTfm gives\n",
            "      (TensorImage of size 4x3x224x224, TensorCategory([2, 7, 7, 2]))\n",
            "    applying Normalize gives\n",
            "      (TensorImage of size 4x3x224x224, TensorCategory([2, 7, 7, 2]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjSXbi6_15YL",
        "colab_type": "text"
      },
      "source": [
        "What we find is it will go through **each** and every single part of our `DataBlock`, test it on an item, and we can see what popped out! **But!** What if we are using the `Datasets` instead? Let's go through how to utilize it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNaWozwJxsdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfms = [[PILImage.create], [parent_label, Categorize()]]\n",
        "item_tfms = [ToTensor(), Resize(128)]\n",
        "batch_tfms = [FlipItem(), RandomResizedCrop(128, min_scale=0.35),\n",
        "              IntToFloatTensor(), Normalize.from_stats(*imagenet_stats, cuda=False)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVMsznX3xZwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "items = get_image_files(path)\n",
        "split_idx = GrandparentSplitter(valid_name='val')(items)\n",
        "dsets = Datasets(items, tfms, splits=split_idx)\n",
        "dls = dsets.dataloaders(after_item=item_tfms, after_batch=batch_tfms, bs=64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecLTeJOi2ES7",
        "colab_type": "text"
      },
      "source": [
        "We'll want to grab the first item from our set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J8q8I1WxxWV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = dsets.train[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmU1p0D7xzoZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9b9619d1-daf2-4054-901f-e1227be196b3"
      },
      "source": [
        "x"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<fastai2.vision.core.PILImage image mode=RGB size=500x333 at 0x7F2A6592CE10>,\n",
              " TensorCategory(3))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVv0loTd2GzP",
        "colab_type": "text"
      },
      "source": [
        "And pass it into any `after_item` or `after_batch` transform `Pipeline`. We can list them by calling them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AP-Bh9Dx3qI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "712a6fa3-4b96-4730-c17d-e757bdd675fb"
      },
      "source": [
        "dls.train.after_item"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline: Resize -> ToTensor"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b318Z4mvyQ-j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "23438797-837e-4087-9782-3872c9bf193c"
      },
      "source": [
        "dls.train.after_batch"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline: FlipItem -> RandomResizedCrop -> IntToFloatTensor -> Normalize"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WogfoDgA2PIn",
        "colab_type": "text"
      },
      "source": [
        "And now we can pass in our item through the `Pipeline` like so:\n",
        "\n",
        "(`x[0]` has our input and `x[1]` has our `y`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY_kdx1BjnOC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "outputId": "c787ba1b-fd9d-4d4b-fcfe-64373382a267"
      },
      "source": [
        "for f in dls.train.after_item:\n",
        "  name = f.name\n",
        "  x = f(x)\n",
        "  print(name, x[0])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Resize TensorImage([[[[ 0.8961,  0.8961,  0.8789,  ...,  0.6563,  0.6221,  0.5536],\n",
            "          [ 0.7248,  0.6906,  0.6563,  ...,  0.6049,  0.6221,  0.6392],\n",
            "          [ 0.4851,  0.4679,  0.4166,  ...,  0.5707,  0.6049,  0.5707],\n",
            "          ...,\n",
            "          [ 0.1254, -0.3198, -0.5253,  ..., -1.7240, -1.8097, -1.7754],\n",
            "          [-0.5253, -1.1075, -1.1932,  ..., -1.4672, -1.7583, -1.6384],\n",
            "          [-0.8507, -1.2617, -0.9877,  ..., -1.5699, -1.6213, -1.7240]],\n",
            "\n",
            "         [[ 1.2381,  1.2381,  1.2206,  ...,  1.0455,  0.9755,  0.9230],\n",
            "          [ 1.0805,  1.0455,  1.0105,  ...,  0.9930,  1.0105,  1.0105],\n",
            "          [ 0.8179,  0.7829,  0.7129,  ...,  0.9230,  0.9755,  0.9405],\n",
            "          ...,\n",
            "          [ 0.5203, -0.1625, -0.5476,  ..., -1.1253, -1.1779, -1.3354],\n",
            "          [ 0.0651, -1.0553, -1.2654,  ..., -1.0728, -1.3179, -1.4055],\n",
            "          [-0.0924, -1.0028, -0.9503,  ..., -1.2304, -1.3880, -1.6155]],\n",
            "\n",
            "         [[ 1.5942,  1.5942,  1.5594,  ...,  1.4025,  1.3328,  1.2805],\n",
            "          [ 1.4548,  1.4025,  1.3502,  ...,  1.3502,  1.3677,  1.3677],\n",
            "          [ 1.1411,  1.1062,  1.0539,  ...,  1.2805,  1.3328,  1.2631],\n",
            "          ...,\n",
            "          [-0.0267, -0.7413, -1.0376,  ..., -1.5604, -1.6127, -1.6302],\n",
            "          [-0.5147, -1.2816, -1.4384,  ..., -1.4210, -1.6476, -1.6476],\n",
            "          [-0.8284, -1.3513, -1.2816,  ..., -1.2990, -1.6302, -1.6824]]]])\n",
            "ToTensor TensorImage([[[[ 0.8961,  0.8961,  0.8789,  ...,  0.6563,  0.6221,  0.5536],\n",
            "          [ 0.7248,  0.6906,  0.6563,  ...,  0.6049,  0.6221,  0.6392],\n",
            "          [ 0.4851,  0.4679,  0.4166,  ...,  0.5707,  0.6049,  0.5707],\n",
            "          ...,\n",
            "          [ 0.1254, -0.3198, -0.5253,  ..., -1.7240, -1.8097, -1.7754],\n",
            "          [-0.5253, -1.1075, -1.1932,  ..., -1.4672, -1.7583, -1.6384],\n",
            "          [-0.8507, -1.2617, -0.9877,  ..., -1.5699, -1.6213, -1.7240]],\n",
            "\n",
            "         [[ 1.2381,  1.2381,  1.2206,  ...,  1.0455,  0.9755,  0.9230],\n",
            "          [ 1.0805,  1.0455,  1.0105,  ...,  0.9930,  1.0105,  1.0105],\n",
            "          [ 0.8179,  0.7829,  0.7129,  ...,  0.9230,  0.9755,  0.9405],\n",
            "          ...,\n",
            "          [ 0.5203, -0.1625, -0.5476,  ..., -1.1253, -1.1779, -1.3354],\n",
            "          [ 0.0651, -1.0553, -1.2654,  ..., -1.0728, -1.3179, -1.4055],\n",
            "          [-0.0924, -1.0028, -0.9503,  ..., -1.2304, -1.3880, -1.6155]],\n",
            "\n",
            "         [[ 1.5942,  1.5942,  1.5594,  ...,  1.4025,  1.3328,  1.2805],\n",
            "          [ 1.4548,  1.4025,  1.3502,  ...,  1.3502,  1.3677,  1.3677],\n",
            "          [ 1.1411,  1.1062,  1.0539,  ...,  1.2805,  1.3328,  1.2631],\n",
            "          ...,\n",
            "          [-0.0267, -0.7413, -1.0376,  ..., -1.5604, -1.6127, -1.6302],\n",
            "          [-0.5147, -1.2816, -1.4384,  ..., -1.4210, -1.6476, -1.6476],\n",
            "          [-0.8284, -1.3513, -1.2816,  ..., -1.2990, -1.6302, -1.6824]]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDI4N_-fx9fq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2e81c396-f8dd-456d-8ca5-d66dc0fd438d"
      },
      "source": [
        "for f in dls.train.after_batch:\n",
        "  name = f.name\n",
        "  x = f(x)\n",
        "  print(name, x[0])"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FlipItem TensorImage([[[176, 176, 175,  ..., 162, 160, 156],\n",
            "         [166, 164, 162,  ..., 159, 160, 161],\n",
            "         [152, 151, 148,  ..., 157, 159, 157],\n",
            "         ...,\n",
            "         [131, 105,  93,  ...,  23,  18,  20],\n",
            "         [ 93,  59,  54,  ...,  38,  21,  28],\n",
            "         [ 74,  50,  66,  ...,  32,  29,  23]],\n",
            "\n",
            "        [[187, 187, 186,  ..., 176, 172, 169],\n",
            "         [178, 176, 174,  ..., 173, 174, 174],\n",
            "         [163, 161, 157,  ..., 169, 172, 170],\n",
            "         ...,\n",
            "         [146, 107,  85,  ...,  52,  49,  40],\n",
            "         [120,  56,  44,  ...,  55,  41,  36],\n",
            "         [111,  59,  62,  ...,  46,  37,  24]],\n",
            "\n",
            "        [[195, 195, 193,  ..., 184, 180, 177],\n",
            "         [187, 184, 181,  ..., 181, 182, 182],\n",
            "         [169, 167, 164,  ..., 177, 180, 176],\n",
            "         ...,\n",
            "         [102,  61,  44,  ...,  14,  11,  10],\n",
            "         [ 74,  30,  21,  ...,  22,   9,   9],\n",
            "         [ 56,  26,  30,  ...,  29,  10,   7]]], dtype=torch.uint8)\n",
            "RandomResizedCrop TensorImage([[[176, 176, 175,  ..., 162, 160, 156],\n",
            "         [166, 164, 162,  ..., 159, 160, 161],\n",
            "         [152, 151, 148,  ..., 157, 159, 157],\n",
            "         ...,\n",
            "         [131, 105,  93,  ...,  23,  18,  20],\n",
            "         [ 93,  59,  54,  ...,  38,  21,  28],\n",
            "         [ 74,  50,  66,  ...,  32,  29,  23]],\n",
            "\n",
            "        [[187, 187, 186,  ..., 176, 172, 169],\n",
            "         [178, 176, 174,  ..., 173, 174, 174],\n",
            "         [163, 161, 157,  ..., 169, 172, 170],\n",
            "         ...,\n",
            "         [146, 107,  85,  ...,  52,  49,  40],\n",
            "         [120,  56,  44,  ...,  55,  41,  36],\n",
            "         [111,  59,  62,  ...,  46,  37,  24]],\n",
            "\n",
            "        [[195, 195, 193,  ..., 184, 180, 177],\n",
            "         [187, 184, 181,  ..., 181, 182, 182],\n",
            "         [169, 167, 164,  ..., 177, 180, 176],\n",
            "         ...,\n",
            "         [102,  61,  44,  ...,  14,  11,  10],\n",
            "         [ 74,  30,  21,  ...,  22,   9,   9],\n",
            "         [ 56,  26,  30,  ...,  29,  10,   7]]], dtype=torch.uint8)\n",
            "IntToFloatTensor TensorImage([[[0.6902, 0.6902, 0.6863,  ..., 0.6353, 0.6275, 0.6118],\n",
            "         [0.6510, 0.6431, 0.6353,  ..., 0.6235, 0.6275, 0.6314],\n",
            "         [0.5961, 0.5922, 0.5804,  ..., 0.6157, 0.6235, 0.6157],\n",
            "         ...,\n",
            "         [0.5137, 0.4118, 0.3647,  ..., 0.0902, 0.0706, 0.0784],\n",
            "         [0.3647, 0.2314, 0.2118,  ..., 0.1490, 0.0824, 0.1098],\n",
            "         [0.2902, 0.1961, 0.2588,  ..., 0.1255, 0.1137, 0.0902]],\n",
            "\n",
            "        [[0.7333, 0.7333, 0.7294,  ..., 0.6902, 0.6745, 0.6627],\n",
            "         [0.6980, 0.6902, 0.6824,  ..., 0.6784, 0.6824, 0.6824],\n",
            "         [0.6392, 0.6314, 0.6157,  ..., 0.6627, 0.6745, 0.6667],\n",
            "         ...,\n",
            "         [0.5725, 0.4196, 0.3333,  ..., 0.2039, 0.1922, 0.1569],\n",
            "         [0.4706, 0.2196, 0.1725,  ..., 0.2157, 0.1608, 0.1412],\n",
            "         [0.4353, 0.2314, 0.2431,  ..., 0.1804, 0.1451, 0.0941]],\n",
            "\n",
            "        [[0.7647, 0.7647, 0.7569,  ..., 0.7216, 0.7059, 0.6941],\n",
            "         [0.7333, 0.7216, 0.7098,  ..., 0.7098, 0.7137, 0.7137],\n",
            "         [0.6627, 0.6549, 0.6431,  ..., 0.6941, 0.7059, 0.6902],\n",
            "         ...,\n",
            "         [0.4000, 0.2392, 0.1725,  ..., 0.0549, 0.0431, 0.0392],\n",
            "         [0.2902, 0.1176, 0.0824,  ..., 0.0863, 0.0353, 0.0353],\n",
            "         [0.2196, 0.1020, 0.1176,  ..., 0.1137, 0.0392, 0.0275]]])\n",
            "Normalize TensorImage([[[[ 0.8961,  0.8961,  0.8789,  ...,  0.6563,  0.6221,  0.5536],\n",
            "          [ 0.7248,  0.6906,  0.6563,  ...,  0.6049,  0.6221,  0.6392],\n",
            "          [ 0.4851,  0.4679,  0.4166,  ...,  0.5707,  0.6049,  0.5707],\n",
            "          ...,\n",
            "          [ 0.1254, -0.3198, -0.5253,  ..., -1.7240, -1.8097, -1.7754],\n",
            "          [-0.5253, -1.1075, -1.1932,  ..., -1.4672, -1.7583, -1.6384],\n",
            "          [-0.8507, -1.2617, -0.9877,  ..., -1.5699, -1.6213, -1.7240]],\n",
            "\n",
            "         [[ 1.2381,  1.2381,  1.2206,  ...,  1.0455,  0.9755,  0.9230],\n",
            "          [ 1.0805,  1.0455,  1.0105,  ...,  0.9930,  1.0105,  1.0105],\n",
            "          [ 0.8179,  0.7829,  0.7129,  ...,  0.9230,  0.9755,  0.9405],\n",
            "          ...,\n",
            "          [ 0.5203, -0.1625, -0.5476,  ..., -1.1253, -1.1779, -1.3354],\n",
            "          [ 0.0651, -1.0553, -1.2654,  ..., -1.0728, -1.3179, -1.4055],\n",
            "          [-0.0924, -1.0028, -0.9503,  ..., -1.2304, -1.3880, -1.6155]],\n",
            "\n",
            "         [[ 1.5942,  1.5942,  1.5594,  ...,  1.4025,  1.3328,  1.2805],\n",
            "          [ 1.4548,  1.4025,  1.3502,  ...,  1.3502,  1.3677,  1.3677],\n",
            "          [ 1.1411,  1.1062,  1.0539,  ...,  1.2805,  1.3328,  1.2631],\n",
            "          ...,\n",
            "          [-0.0267, -0.7413, -1.0376,  ..., -1.5604, -1.6127, -1.6302],\n",
            "          [-0.5147, -1.2816, -1.4384,  ..., -1.4210, -1.6476, -1.6476],\n",
            "          [-0.8284, -1.3513, -1.2816,  ..., -1.2990, -1.6302, -1.6824]]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdJXrXhi2Tz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}