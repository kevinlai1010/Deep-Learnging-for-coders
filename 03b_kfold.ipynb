{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "k-fold.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWl-5XI0kElY",
        "colab_type": "text"
      },
      "source": [
        "# Notebook 3b: K-Fold Validation\n",
        "\n",
        "In this notebook I will show you how to implement K-Fold Cross Validation on your data and apply this to a test set. We will use the ADULTs dataset as an example but the steps can be applied across the board"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1x8_8rEczoL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install torch===1.3.0 torchvision===0.4.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install git+https://github.com/fastai/fastai_dev > /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nVPQXt0dETU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai2.torch_basics import *\n",
        "from fastai2.basics import *\n",
        "from fastai2.tabular.core import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8vuRa4sdGEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = untar_data(URLs.ADULT_SAMPLE)\n",
        "df = pd.read_csv(path/'adult.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcaIvQ0AdHIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']\n",
        "cont_names = ['age', 'fnlwgt', 'education-num']\n",
        "procs = [Categorify, FillMissing, Normalize]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di5eXG-TkRN5",
        "colab_type": "text"
      },
      "source": [
        "First I want to seperate a test set that is the last 10% of my data (for adults this is fine, but in actuality this is quite an important topic. To read more see [here](https://www.fast.ai/2017/11/13/validation-sets/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-Nm-OtLdKOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "end = len(df) - 3256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxHq9ezedmux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = df.iloc[end:]\n",
        "train = df.iloc[:end]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH1xBuTpo-46",
        "colab_type": "text"
      },
      "source": [
        "Now let's grab `StratifiedKFold` from the `sklearn` library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5B2mxeUjdpuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18ZWfEfBpDjF",
        "colab_type": "text"
      },
      "source": [
        "The following code is just to get our `TabularLearner` up and running"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yQLlMy_gnPX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def emb_sz_rule(n_cat): \n",
        "    \"Rule of thumb to pick embedding size corresponding to `n_cat`\"\n",
        "    return min(600, round(1.6 * n_cat**0.56))\n",
        "\n",
        "def _one_emb_sz(classes, n, sz_dict=None):\n",
        "    \"Pick an embedding size for `n` depending on `classes` if not given in `sz_dict`.\"\n",
        "    sz_dict = ifnone(sz_dict, {})\n",
        "    n_cat = len(classes[n])\n",
        "    sz = sz_dict.get(n, int(emb_sz_rule(n_cat)))  # rule of thumb\n",
        "    return n_cat,sz\n",
        "\n",
        "def get_emb_sz(to, sz_dict=None):\n",
        "    \"Get default embedding size from `TabularPreprocessor` `proc` or the ones in `sz_dict`\"\n",
        "    return [_one_emb_sz(to.procs.classes, n, sz_dict) for n in to.cat_names]\n",
        "\n",
        "class TabularModel(Module):\n",
        "    \"Basic model for tabular data.\"\n",
        "    def __init__(self, emb_szs, n_cont, out_sz, layers, ps=None, embed_p=0., y_range=None, use_bn=True, bn_final=False):\n",
        "        ps = ifnone(ps, [0]*len(layers))\n",
        "        if not is_listy(ps): ps = [ps]*len(layers)\n",
        "        self.embeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in emb_szs])\n",
        "        self.emb_drop = nn.Dropout(embed_p)\n",
        "        self.bn_cont = nn.BatchNorm1d(n_cont)\n",
        "        n_emb = sum(e.embedding_dim for e in self.embeds)\n",
        "        self.n_emb,self.n_cont,self.y_range = n_emb,n_cont,y_range\n",
        "        sizes = [n_emb + n_cont] + layers + [out_sz]\n",
        "        actns = [nn.ReLU(inplace=True) for _ in range(len(sizes)-2)] + [None]\n",
        "        _layers = [BnDropLin(sizes[i], sizes[i+1], bn=use_bn and i!=0, p=p, act=a)\n",
        "                       for i,(p,a) in enumerate(zip([0.]+ps,actns))]\n",
        "        if bn_final: _layers.append(nn.BatchNorm1d(sizes[-1]))\n",
        "        self.layers = nn.Sequential(*_layers)\n",
        "    \n",
        "    def forward(self, x_cat, x_cont):\n",
        "        if self.n_emb != 0:\n",
        "            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n",
        "            x = torch.cat(x, 1)\n",
        "            x = self.emb_drop(x)\n",
        "        if self.n_cont != 0:\n",
        "            x_cont = self.bn_cont(x_cont)\n",
        "            x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n",
        "        x = self.layers(x)\n",
        "        if self.y_range is not None:\n",
        "            x = (self.y_range[1]-self.y_range[0]) * torch.sigmoid(x) + self.y_range[0]\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGklhaB8iZ5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# My own mock version of a `tabular_learner`\n",
        "def tabular_learner(data:DataBunch, layers, emb_szs=None, metrics=None,\n",
        "        ps=None, emb_drop:float=0., y_range=None, use_bn:bool=True, **learn_kwargs):\n",
        "    \"Get a `Learner` using `data`, with `metrics`, including a `TabularModel` created using the remaining params.\"\n",
        "    emb_szs = get_emb_sz(data.train)\n",
        "    model = TabularModel(emb_szs, len(data.cont_names), data.train[data.train.y_names].nunique(), layers=layers)\n",
        "    return Learner(data, model, metrics=metrics, **learn_kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymxSmCwjpIm7",
        "colab_type": "text"
      },
      "source": [
        "Now for the actual running. I'll describe what we're doing below step by step. We declare our `cat` and `cont` vars, our procs, and also generate our test set's data loader (so we can test against it). Along wtih this, to stay in v2 style our validation and test lists will be of type `L`.\n",
        "\n",
        "From here, we will use the `StratifiedKFold` to generate 10 shuffled splits, and split them with the `.split` method. From here, we can go into each of those splits and they will contain our indexs. Convert them to `L`'s and we can directly pass them into our `TabularPandas`. From here, we create our `DataBunch`, `Learner`, train it, and then evaluate on our test data.\n",
        "\n",
        "Finally, we will print out the validation and test set statistics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPs6ptQ8eXfN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "f6490e26-b30d-43b6-a5c3-8a57cda1e6ad"
      },
      "source": [
        "val_pct = L()\n",
        "test_pct = L()\n",
        "\n",
        "test_preds = L()\n",
        "\n",
        "cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']\n",
        "cont_names = ['age', 'fnlwgt', 'education-num']\n",
        "procs = [Categorify, FillMissing, Normalize]\n",
        "\n",
        "test_dl = TabularPandas(test, procs, cat_names, cont_names, y_names=\"salary\")\n",
        "test_dl = TabDataLoader(test_dl)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
        "res = skf.split(train.index, train['salary'])\n",
        "for x, y in res:\n",
        "  ix = (L(list(x)), L(list(y)))\n",
        "  to = TabularPandas(train, procs, cat_names, cont_names, y_names=\"salary\", splits=ix)\n",
        "  data = to.databunch()\n",
        "  learn = tabular_learner(data, layers=[200,100], loss_func=CrossEntropyLossFlat(), metrics=accuracy)\n",
        "  learn.fit(1)\n",
        "  val_pct.append(learn.validate()[1])\n",
        "  test_pct.append(learn.validate(dl=test_dl)[1])"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(#5) [0,0.39185065031051636,0.36321401596069336,0.8341862559318542,00:41]\n",
            "(#5) [0,0.3853352665901184,0.370516300201416,0.8184919953346252,00:40]\n",
            "(#5) [0,0.39923539757728577,0.37282225489616394,0.8375980854034424,00:41]\n",
            "(#5) [0,0.3787161111831665,0.3603558838367462,0.837256908416748,00:41]\n",
            "(#5) [0,0.36787909269332886,0.3505774438381195,0.8365745544433594,00:41]\n",
            "(#5) [0,0.3749101459980011,0.37149032950401306,0.8320819139480591,00:40]\n",
            "(#5) [0,0.3706760108470917,0.37037286162376404,0.8245733976364136,00:41]\n",
            "(#5) [0,0.36982911825180054,0.3898116648197174,0.8156996369361877,00:41]\n",
            "(#5) [0,0.3981660306453705,0.36508622765541077,0.8313993215560913,00:40]\n",
            "(#5) [0,0.3743650019168854,0.3596053421497345,0.8327645063400269,00:40]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc5JrtMKm2GI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "e3d505f7-b260-4a7a-eeba-eed3ce2564bb"
      },
      "source": [
        "print(f'Validation:\\nmean: {np.mean(val_pct)}\\nstd: {np.std(val_pct)}')\n",
        "print(f'\\n\\nTest:\\nmean: {np.mean(test_pct)}\\nstd: {np.std(test_pct)}')"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation:\n",
            "mean: 0.8300626575946808\n",
            "std: 0.007425775359093289\n",
            "\n",
            "\n",
            "Test:\n",
            "mean: 0.7957616686820984\n",
            "std: 0.014733064799056943\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK32Fc2sswqF",
        "colab_type": "text"
      },
      "source": [
        "## Bonus:\n",
        "\n",
        "If we wanted to do a mash up of our ten models, here is how you would adjust the loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx4OWi9Es2az",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "7df47928-6335-433a-9e5c-11d283151151"
      },
      "source": [
        "test_preds = L() # HERE\n",
        "\n",
        "cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']\n",
        "cont_names = ['age', 'fnlwgt', 'education-num']\n",
        "procs = [Categorify, FillMissing, Normalize]\n",
        "\n",
        "test_dl = TabularPandas(test, procs, cat_names, cont_names, y_names=\"salary\")\n",
        "test_dl = TabDataLoader(test_dl)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
        "res = skf.split(train.index, train['salary'])\n",
        "for x, y in res:\n",
        "  ix = (L(list(x)), L(list(y)))\n",
        "  to = TabularPandas(train, procs, cat_names, cont_names, y_names=\"salary\", splits=ix)\n",
        "  data = to.databunch()\n",
        "  learn = tabular_learner(data, layers=[200,100], loss_func=CrossEntropyLossFlat(), metrics=accuracy)\n",
        "  learn.fit(1)\n",
        "  test_preds.append(learn.get_preds(dl=test_dl)[0]) # HERE"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(#5) [0,0.3765460252761841,0.3636363744735718,0.8358922004699707,00:40]\n",
            "(#5) [0,0.3646171987056732,0.35906463861465454,0.8249744176864624,00:40]\n",
            "(#5) [0,0.3781249523162842,0.35186585783958435,0.8355510234832764,00:40]\n",
            "(#5) [0,0.40145716071128845,0.3564353585243225,0.8311156630516052,00:40]\n",
            "(#5) [0,0.38083574175834656,0.3732168674468994,0.8348686695098877,00:40]\n",
            "(#5) [0,0.4260295331478119,0.39271262288093567,0.8249146938323975,00:40]\n",
            "(#5) [0,0.36830320954322815,0.3626205623149872,0.8273037672042847,00:40]\n",
            "(#5) [0,0.3807460367679596,0.36837369203567505,0.8331058025360107,00:40]\n",
            "(#5) [0,0.40952637791633606,0.35914427042007446,0.8307167291641235,00:40]\n",
            "(#5) [0,0.37534549832344055,0.359743595123291,0.8313993215560913,00:40]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sc3yGHybu10J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = [pred for pred in test_preds]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IMMdpJQsK68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = sum(preds)/10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOEz_qfxvJGW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "62c6631f-2001-4ef7-e049-fc842948b80a"
      },
      "source": [
        "accuracy(pred, test_preds[0][1])"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.8117)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJJ1leI3qESS",
        "colab_type": "text"
      },
      "source": [
        "And we're done! 20 lines of code! *Much* easier to do in v2 than v1 thanks to that test `DataLoader` being so simple to set up."
      ]
    }
  ]
}
